{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32ef7bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\somya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\somya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tok = WordPunctTokenizer()\n",
    "\n",
    "# sklearn\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddd5be93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('combined_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "908b140a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Tweet Id</th>\n",
       "      <th>Username</th>\n",
       "      <th>Text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1245138728135385089</td>\n",
       "      <td>trippyhippie99</td>\n",
       "      <td>For someone who struggles with anxiety this pa...</td>\n",
       "      <td>someone struggles anxiety pandemic terrifying ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1245116161223778306</td>\n",
       "      <td>KTABTV</td>\n",
       "      <td>Report: 31% of people sleeping less due to COV...</td>\n",
       "      <td>report people sleeping less due covid related ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1245116161634820096</td>\n",
       "      <td>bigcountryhome</td>\n",
       "      <td>Report: 31% of people sleeping less due to COV...</td>\n",
       "      <td>report people sleeping less due covid related ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1245116168224022532</td>\n",
       "      <td>futureof_school</td>\n",
       "      <td>Anxiety isn't an unusual feeling or reaction t...</td>\n",
       "      <td>anxiety unusual feeling reaction parents stude...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1245116301778927616</td>\n",
       "      <td>CaponeTeaches</td>\n",
       "      <td>@hauber_alex For me, last week it was anxiety....</td>\n",
       "      <td>alex last week anxiety news first friend dying...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16153</th>\n",
       "      <td>0</td>\n",
       "      <td>1696043380</td>\n",
       "      <td>Irv25</td>\n",
       "      <td>Good Morning twitters!!! I am soooo tired...I ...</td>\n",
       "      <td>good morning twitters soooo tired really need ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16154</th>\n",
       "      <td>0</td>\n",
       "      <td>2050960500</td>\n",
       "      <td>Smalltalkwitht</td>\n",
       "      <td>@JoyceSchneider1  It's so true... I did!</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16155</th>\n",
       "      <td>0</td>\n",
       "      <td>1880015787</td>\n",
       "      <td>jaySics</td>\n",
       "      <td>i want to go see Terminator again. goodnight</td>\n",
       "      <td>want see terminator goodnight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16156</th>\n",
       "      <td>0</td>\n",
       "      <td>2016680792</td>\n",
       "      <td>Mzpurrfection</td>\n",
       "      <td>@popnbulletz1 SEE U 2  whats new</td>\n",
       "      <td>see whats new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16157</th>\n",
       "      <td>0</td>\n",
       "      <td>1827454492</td>\n",
       "      <td>GabbySibaja</td>\n",
       "      <td>just like i promised, 1 more follower today &amp;a...</td>\n",
       "      <td>promised follower today guys get shout videooo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16158 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Label             Tweet Id         Username  \\\n",
       "0          1  1245138728135385089   trippyhippie99   \n",
       "1          1  1245116161223778306           KTABTV   \n",
       "2          1  1245116161634820096   bigcountryhome   \n",
       "3          1  1245116168224022532  futureof_school   \n",
       "4          1  1245116301778927616    CaponeTeaches   \n",
       "...      ...                  ...              ...   \n",
       "16153      0           1696043380            Irv25   \n",
       "16154      0           2050960500   Smalltalkwitht   \n",
       "16155      0           1880015787          jaySics   \n",
       "16156      0           2016680792    Mzpurrfection   \n",
       "16157      0           1827454492      GabbySibaja   \n",
       "\n",
       "                                                    Text  \\\n",
       "0      For someone who struggles with anxiety this pa...   \n",
       "1      Report: 31% of people sleeping less due to COV...   \n",
       "2      Report: 31% of people sleeping less due to COV...   \n",
       "3      Anxiety isn't an unusual feeling or reaction t...   \n",
       "4      @hauber_alex For me, last week it was anxiety....   \n",
       "...                                                  ...   \n",
       "16153  Good Morning twitters!!! I am soooo tired...I ...   \n",
       "16154           @JoyceSchneider1  It's so true... I did!   \n",
       "16155      i want to go see Terminator again. goodnight    \n",
       "16156                   @popnbulletz1 SEE U 2  whats new   \n",
       "16157  just like i promised, 1 more follower today &a...   \n",
       "\n",
       "                                              clean_text  \n",
       "0      someone struggles anxiety pandemic terrifying ...  \n",
       "1      report people sleeping less due covid related ...  \n",
       "2      report people sleeping less due covid related ...  \n",
       "3      anxiety unusual feeling reaction parents stude...  \n",
       "4      alex last week anxiety news first friend dying...  \n",
       "...                                                  ...  \n",
       "16153  good morning twitters soooo tired really need ...  \n",
       "16154                                               true  \n",
       "16155                      want see terminator goodnight  \n",
       "16156                                      see whats new  \n",
       "16157     promised follower today guys get shout videooo  \n",
       "\n",
       "[16158 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22dc471a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Label             Tweet Id        Username  \\\n",
      "1196       1  1267194163319246851         DrHyken   \n",
      "1269       1  1267234357237837824      AlexTelman   \n",
      "4218       1  1333544792941228032     alven334_al   \n",
      "6576       1  1388177198494871552       MAC280215   \n",
      "8329       0           1826246928   xxlilokittehh   \n",
      "...      ...                  ...             ...   \n",
      "15438      0           1558449271           MaLo8   \n",
      "15749      0           2051417706      lilmo4ever   \n",
      "15762      0           1553398707       Trish1981   \n",
      "15958      0           1824584445     ReemerPromo   \n",
      "15988      0           1957040606  tiffunnyfranco   \n",
      "\n",
      "                                                    Text clean_text  \n",
      "1196   How to #Prepare for #Life after #Quarantine. #...        NaN  \n",
      "1269   My #wish for #youðŸ˜€is \\n#love &amp; #joy\\n\\n#he...        NaN  \n",
      "4218   Awwww this is so freaking cute @The_WorldWeWan...        NaN  \n",
      "6576             #Covid the #Great #Depression by #China        NaN  \n",
      "8329                                @AddictionLove same         NaN  \n",
      "...                                                  ...        ...  \n",
      "15438                           @forrestfanatic thanks!         NaN  \n",
      "15749  @V_A_ awwww, I had 2 talk 2 the Lord 2day a lo...        NaN  \n",
      "15762                    @MajorDodson awwww...how sweet         NaN  \n",
      "15958  www.reemerpromo.co.uk/competitions.html     lo...        NaN  \n",
      "15988                             @tiffanylue Ok thanks         NaN  \n",
      "\n",
      "[85 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print (df[df['clean_text'].isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2481f866",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna()\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "169756a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Label, Tweet Id, Username, Text, clean_text]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print (df[df['clean_text'].isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54b8c5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of the data is:         16073\n",
      "No. of positive tagged sentences is:  7795\n",
      "No. of negative tagged sentences is: 8278\n"
     ]
    }
   ],
   "source": [
    "positives = df['Label'][df.Label == 0 ]\n",
    "negatives = df['Label'][df.Label == 1 ]\n",
    "\n",
    "print('Total length of the data is:         {}'.format(df.shape[0]))\n",
    "print('No. of positive tagged sentences is:  {}'.format(len(positives)))\n",
    "print('No. of negative tagged sentences is: {}'.format(len(negatives)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "625d094e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Distribution of Data')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x=\"Label\", hue=\"Label\", dodge=False, data=df).set_title(\"Distribution of Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03d02f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "df =  shuffle(df).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3c79830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Tweet Id</th>\n",
       "      <th>Username</th>\n",
       "      <th>Text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1377377457037185025</td>\n",
       "      <td>ItsDeeRenee12</td>\n",
       "      <td>@AdamantxYves I understand this feeling comple...</td>\n",
       "      <td>understand feeling completely anxiety around g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1245113705085513728</td>\n",
       "      <td>VippusaO</td>\n",
       "      <td>MIXED MESSAGES COMING FROM WH IS CAUSING ANXIE...</td>\n",
       "      <td>mixed messages coming causing anxiety panic tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1564960153</td>\n",
       "      <td>alienzki</td>\n",
       "      <td>http://twitpic.com/3nhpr - Daxene got works to...</td>\n",
       "      <td>daxene got works paper works stuffs ahihi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2013933734</td>\n",
       "      <td>556KaraB</td>\n",
       "      <td>Packing for a Trip this weekend</td>\n",
       "      <td>packing trip weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1333511486879232000</td>\n",
       "      <td>___mMx</td>\n",
       "      <td>@_justkimberleyx I was alright at them first l...</td>\n",
       "      <td>alright first lockdown trying last weeks get b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16068</th>\n",
       "      <td>0</td>\n",
       "      <td>2070808803</td>\n",
       "      <td>mimgodfather</td>\n",
       "      <td>@christine_downs Happy Birthday you</td>\n",
       "      <td>downs happy birthday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16069</th>\n",
       "      <td>0</td>\n",
       "      <td>1997067641</td>\n",
       "      <td>mariavenegas</td>\n",
       "      <td>#musicmonday Everyone follow @heymonday  Love ...</td>\n",
       "      <td>everyone follow love music great band</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16070</th>\n",
       "      <td>0</td>\n",
       "      <td>2189006788</td>\n",
       "      <td>christenxo</td>\n",
       "      <td>On bed rest =( had a scare last night the baby...</td>\n",
       "      <td>bed rest scare last night baby coming good thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16071</th>\n",
       "      <td>0</td>\n",
       "      <td>1933096608</td>\n",
       "      <td>JillHenninger</td>\n",
       "      <td>Starting to get ready for Seattle - can't wait...</td>\n",
       "      <td>starting get ready seattle wait see girls days...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16072</th>\n",
       "      <td>1</td>\n",
       "      <td>1366159226637344768</td>\n",
       "      <td>Ajohms1956</td>\n",
       "      <td>Just depressed myself by checking out the numb...</td>\n",
       "      <td>depressed checking number local bars restauran...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16073 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Label             Tweet Id       Username  \\\n",
       "0          1  1377377457037185025  ItsDeeRenee12   \n",
       "1          1  1245113705085513728       VippusaO   \n",
       "2          0           1564960153       alienzki   \n",
       "3          0           2013933734       556KaraB   \n",
       "4          1  1333511486879232000         ___mMx   \n",
       "...      ...                  ...            ...   \n",
       "16068      0           2070808803   mimgodfather   \n",
       "16069      0           1997067641   mariavenegas   \n",
       "16070      0           2189006788     christenxo   \n",
       "16071      0           1933096608  JillHenninger   \n",
       "16072      1  1366159226637344768     Ajohms1956   \n",
       "\n",
       "                                                    Text  \\\n",
       "0      @AdamantxYves I understand this feeling comple...   \n",
       "1      MIXED MESSAGES COMING FROM WH IS CAUSING ANXIE...   \n",
       "2      http://twitpic.com/3nhpr - Daxene got works to...   \n",
       "3                       Packing for a Trip this weekend    \n",
       "4      @_justkimberleyx I was alright at them first l...   \n",
       "...                                                  ...   \n",
       "16068               @christine_downs Happy Birthday you    \n",
       "16069  #musicmonday Everyone follow @heymonday  Love ...   \n",
       "16070  On bed rest =( had a scare last night the baby...   \n",
       "16071  Starting to get ready for Seattle - can't wait...   \n",
       "16072  Just depressed myself by checking out the numb...   \n",
       "\n",
       "                                              clean_text  \n",
       "0      understand feeling completely anxiety around g...  \n",
       "1      mixed messages coming causing anxiety panic tr...  \n",
       "2              daxene got works paper works stuffs ahihi  \n",
       "3                                   packing trip weekend  \n",
       "4      alright first lockdown trying last weeks get b...  \n",
       "...                                                  ...  \n",
       "16068                               downs happy birthday  \n",
       "16069              everyone follow love music great band  \n",
       "16070  bed rest scare last night baby coming good thi...  \n",
       "16071  starting get ready seattle wait see girls days...  \n",
       "16072  depressed checking number local bars restauran...  \n",
       "\n",
       "[16073 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df2a0ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df['clean_text'], df['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce11faed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91195fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Split done.\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size = 0.20, random_state = 19)\n",
    "print(f'Data Split done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9dbf1c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Naive Bayes\n",
    "def nb_classifier(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    nb =  Pipeline([('vect', CountVectorizer()),\n",
    "                  ('tfidf', TfidfTransformer()),\n",
    "                  ('clf', MultinomialNB()),\n",
    "                 ])\n",
    "\n",
    "    nb.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = nb.predict(X_test)\n",
    "\n",
    "    print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abde2597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9262830482115085\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.85      0.92      1510\n",
      "           1       0.88      0.99      0.93      1705\n",
      "\n",
      "    accuracy                           0.93      3215\n",
      "   macro avg       0.94      0.92      0.93      3215\n",
      "weighted avg       0.93      0.93      0.93      3215\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_classifier(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97b3a412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vaderSentiment in c:\\users\\somya\\anaconda3\\lib\\site-packages (3.3.2)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: requests in c:\\users\\somya\\anaconda3\\lib\\site-packages (from vaderSentiment) (2.25.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\somya\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\somya\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\somya\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\somya\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2020.12.5)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19d6114a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\somya\\anaconda3\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\somya\\anaconda3\\lib\\site-packages (from textblob) (3.6.1)\n",
      "Requirement already satisfied: regex in c:\\users\\somya\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2021.4.4)\n",
      "Requirement already satisfied: click in c:\\users\\somya\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\somya\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.0.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\somya\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.59.0)\n"
     ]
    }
   ],
   "source": [
    "# Install textblob library\n",
    "!pip install textblob\n",
    "\n",
    "# Import TextBlob module\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02c2a086",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\somya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download the lexicon\n",
    "nltk.download(\"vader_lexicon\")\n",
    "\n",
    "# Import the lexicon \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Create an instance of SentimentIntensityAnalyzer\n",
    "sent_analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f287703b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_VADER_output(output_dict):\n",
    "  \n",
    "  polarity = 0\n",
    "\n",
    "  if(output_dict['compound']>= 0.05):\n",
    "    polarity = 0 #positive polarity\n",
    "\n",
    "  elif(output_dict['compound']<= -0.05):\n",
    "    polarity = 1 #negative polarity\n",
    "\n",
    "  return polarity\n",
    "\n",
    "def predict_VADER_sentiment(text):\n",
    "  \n",
    "  output_dict =  sent_analyzer.polarity_scores(text)\n",
    "  return format_VADER_output(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8abde23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_TextBlob_output(prediction):\n",
    "  \n",
    "  polarity = 0\n",
    "\n",
    "  if(prediction.polarity > 0):\n",
    "    polarity = 0 #positive polarity\n",
    "\n",
    "  elif(prediction.polarity < 0):\n",
    "    polarity = 1 #negative polarity\n",
    "\n",
    "  return polarity\n",
    "\n",
    "def predict_Textblob_sentiment(text):\n",
    "  \n",
    "  text_blob_prediction =  TextBlob(text).sentiment\n",
    "  return format_TextBlob_output(text_blob_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c755ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Scores\n",
      "- VADER: 0.8045168916817023\n",
      "- Textblob: 0.599079201144777\n",
      "Classification report\n",
      "- VADER\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.90      0.82      7795\n",
      "           1       0.88      0.72      0.79      8278\n",
      "\n",
      "    accuracy                           0.80     16073\n",
      "   macro avg       0.82      0.81      0.80     16073\n",
      "weighted avg       0.82      0.80      0.80     16073\n",
      "\n",
      "\n",
      "\n",
      "- TextBlob\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.90      0.68      7795\n",
      "           1       0.77      0.32      0.45      8278\n",
      "\n",
      "    accuracy                           0.60     16073\n",
      "   macro avg       0.66      0.61      0.57     16073\n",
      "weighted avg       0.66      0.60      0.56     16073\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "## VADER SECTION\n",
    "df[\"vader_prediction\"] = df[\"clean_text\"].apply(predict_VADER_sentiment)\n",
    "vader_accuracy = accuracy_score(df['Label'], df['vader_prediction'])\n",
    "\n",
    "## TextBlob SECTION\n",
    "df[\"textblob_prediction\"] = df[\"clean_text\"].apply(predict_Textblob_sentiment)\n",
    "textblob_accuracy = accuracy_score(df['Label'], df['textblob_prediction'])\n",
    "\n",
    "## Print the results\n",
    "print(\"Accuracy Scores\")\n",
    "print(\"- VADER: {}\".format(vader_accuracy))\n",
    "print(\"- Textblob: {}\".format(textblob_accuracy))\n",
    "\n",
    "\n",
    "print(\"Classification report\")\n",
    "print(\"- VADER\")\n",
    "print(classification_report(df['Label'], df['vader_prediction']))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"- TextBlob\")\n",
    "print(classification_report(df['Label'], df['textblob_prediction']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cedc00b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectoriser fitted.\n"
     ]
    }
   ],
   "source": [
    "vectoriser = Pipeline([('vect', CountVectorizer()),\n",
    "                  ('tfidf', TfidfTransformer())])\n",
    "vectoriser.fit(X_train)\n",
    "print(f'Vectoriser fitted.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eebe29c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('vectoriser.pickle','wb')\n",
    "pickle.dump(vectoriser, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e8a3875",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state = 19)\n",
    "model =  Pipeline([('vect', CountVectorizer()),\n",
    "                  ('tfidf', TfidfTransformer()),\n",
    "                  ('clf', MultinomialNB()),\n",
    "                 ])\n",
    "mnb = model.fit(X_train, y_train)\n",
    "pickle.dump(mnb,open(\"mnb.bin\",'wb'))\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ee72c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Transformed.\n"
     ]
    }
   ],
   "source": [
    "X_train = vectoriser.transform(X_train)\n",
    "X_test  = vectoriser.transform(X_test)\n",
    "print(f'Data Transformed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ea06498",
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions_dict = { \n",
    "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is / how does\",\n",
    "\"I'd\": \"I had / I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I shall / I will\",\n",
    "\"I'll've\": \"I shall have / I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80c1a28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "def expand_contractions(s, contractions_dict=contractions_dict):\n",
    "     def replace(match):\n",
    "        return contractions_dict[match.group(0)]\n",
    "     return contractions_re.sub(replace, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "07566039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ftfy in c:\\users\\somya\\anaconda3\\lib\\site-packages (6.1.1)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in c:\\users\\somya\\anaconda3\\lib\\site-packages (from ftfy) (0.2.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install ftfy\n",
    "import ftfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b86223b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "47f06c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.update((\"mon\",\"tue\",\"wed\",\"thu\",\"fri\",\"sat\",\"sun\",\"sunday\",\"monday\",\"tuesday\",\"thursday\",\"friday\",\"saturday\",\"sunday\",\"thurs\",\"thur\",\"tues\"))\n",
    "stop_words.update((\"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\"july\",\"august\",\n",
    "              \"september\",\"october\",\"november\",\"december\",\"jan\",\"feb\",\"mar\",\"apr\",\n",
    "              \"may\",\"jun\",\"jul\",\"aug\",\"sep\",\"oct\",\"nov\",\"dec\", \"twitter\", \"thanking\",\"thanks\",\"fuck\",\"fucking\", \"might\",\"like\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d519c62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(tweets):\n",
    "    cleaned_tweets = []\n",
    "    for tweet in tweets:\n",
    "            tweet = str(tweet)\n",
    "        # if url links, then don't append to avoid news articles, etc.\n",
    "        # Check tweet length, save those > 6 (length of word \"lonely\")\n",
    "            tweet = tweet.lower()\n",
    "\n",
    "       \n",
    "            tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "            tweet = re.sub(r'www.+', \"\", tweet)\n",
    "            tweet = re.sub('[0-9]+', '', tweet)\n",
    "\n",
    "            #remove hashtags, @mention, emoji and image URLs\n",
    "            tweet = ' '.join(re.sub(\"(@[a-z0-9]+)|(\\#[a-z0-9]+)|(<Emoji:.*>)|(pic\\.twitter\\.com\\/.*)\", \" \", tweet).split())\n",
    "            # Remove HTML special entities (e.g. &amp;)\n",
    "            tweet = re.sub(r'\\&\\w*;', '', tweet)\n",
    "            #Convert @username to AT_USER\n",
    "            tweet = re.sub('@[^\\s]+','',tweet)\n",
    "            # Remove tickers\n",
    "            tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "            # Remove hyperlinks\n",
    "            tweet = re.sub(r'https?:\\/\\/.*\\/\\w*', '', tweet)\n",
    "            # Remove words with 2 or fewer letters\n",
    "            tweet = re.sub(r'\\b\\w{1,2}\\b', '', tweet)\n",
    "            # Remove whitespace (including new line characters)\n",
    "            tweet = re.sub(r'\\s\\s+', ' ', tweet)\n",
    "            # Remove single space remaining at the front of the tweet.\n",
    "            tweet = tweet.lstrip(' ') \n",
    "            # Remove characters beyond Basic Multilingual Plane (BMP) of Unicode:\n",
    "            tweet = ''.join(c for c in tweet if c <= '\\uFFFF')\n",
    "            #fix weirdly encoded texts\n",
    "            tweet = ftfy.fix_text(tweet)\n",
    "            #expand contraction\n",
    "            tweet = expand_contractions(tweet)\n",
    "            #remove punctuation\n",
    "            tweet = ' '.join(re.sub(\"([^0-9A-Za-z \\t])\", \" \", tweet).split())\n",
    "            \n",
    "            \n",
    "            # Tokenize and join to remove unneccessary white spaces\n",
    "            words = [x for x  in tok.tokenize(tweet) if len(x) > 1]\n",
    "            #return (\" \".join(words)).strip()\n",
    "\n",
    "                    \n",
    "    \n",
    "            word_tokens = nltk.word_tokenize(tweet) \n",
    "            filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "            tweet = ' '.join(filtered_sentence)\n",
    "\n",
    "            #stemming words\n",
    "            tweet = PorterStemmer().stem(tweet)\n",
    "            \n",
    "            cleaned_tweets.append(tweet)\n",
    "\n",
    "    return cleaned_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a9568e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    '''\n",
    "    Replace '..path/' by the path of the saved models.\n",
    "    '''\n",
    "    \n",
    "    # Load the model.\n",
    "    file = open(\"C:/Users/somya/Downloads/mnb.bin\", 'rb')\n",
    "    mnb = pickle.load(file)\n",
    "    file.close()\n",
    "   \n",
    "    \n",
    "    return mnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1df0c5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text sentiment\n",
      "0                                     I hate twitter  Negative\n",
      "1  @BullofJohn @Shawnifee Am a mech engr n lost m...  Negative\n",
      "2  @MetalMikey Hehe, true that! It was just a rea...  Positive\n",
      "3                                     I love my life  Positive\n",
      "4  I feel like this sounds bad, but my depression...  Negative\n"
     ]
    }
   ],
   "source": [
    "def predict(model, text):\n",
    "    # Predict the sentiment\n",
    "    textdata = clean_tweets(text)\n",
    "    sentiment = model.predict(textdata)\n",
    "    \n",
    "    # Make a list of text with sentiment.\n",
    "    data = []\n",
    "    for text, pred in zip(text, sentiment):\n",
    "        data.append((text,pred))\n",
    "        \n",
    "    # Convert the list into a Pandas DataFrame.\n",
    "    df = pd.DataFrame(data, columns = ['text','sentiment'])\n",
    "    df = df.replace([0,1], [\"Positive\",\"Negative\"])\n",
    "    return df\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # Loading the models.\n",
    "    mnb = load_models()\n",
    "    \n",
    "    # Text to classify should be in a list.\n",
    "    text = [\"I hate twitter\",\n",
    "            \"@BullofJohn @Shawnifee Am a mech engr n lost my job during this pandemic and since then feeding v been a very big issue for me. Pls am nt begging for food though I won't still reject if given but my major need now is a job even if it's a casual job or driving.pls help me out. Am dying of depression.\",\n",
    "            \"@MetalMikey Hehe, true that! It was just a really cool ride, funny and exciting all the way through. Even the stuff I feared was cool.\",\n",
    "            \"I love my life\",\n",
    "            \"I feel like this sounds bad, but my depression is getting to the point where I'd rather get COVID than have to deal with my mental health\"]\n",
    "    \n",
    "    df = predict(mnb, text)\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba010824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73549d85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53d85e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04e79c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
